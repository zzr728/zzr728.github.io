<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rui&#39;s blog</title>
  
  <subtitle>home page</subtitle>
  <link href="https://rui728.com/atom.xml" rel="self"/>
  
  <link href="https://rui728.com/"/>
  <updated>2022-01-26T08:19:54.915Z</updated>
  <id>https://rui728.com/</id>
  
  <author>
    <name>Zhang Ze Rui</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>deepflux详解</title>
    <link href="https://rui728.com/2022/01/26/deepflux/"/>
    <id>https://rui728.com/2022/01/26/deepflux/</id>
    <published>2022-01-26T04:22:33.424Z</published>
    <updated>2022-01-26T08:19:54.915Z</updated>
    
    <content type="html"><![CDATA[<p>The first blog is the detailed annotation of deepflux，which appears in IJCV. And its link is <a href="https://doi.org/10.1007/s11263-021-01430-6">here</a>.</p><p>The main reason I select this paper is the author is my mentor  and I have read it recently.</p><h2 id="Let’s-go"><a href="#Let’s-go" class="headerlink" title="Let’s go!"></a>Let’s go!</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction:"></a>Introduction:</h3><p>​        Skeletonization algorithms provide a concise and  effective representation of deformable objects, while supporting many applications, including object recognition and retrieval,pose estimation, balabala….（就是骨架检测能用到很多方面的运用，所以研究很重要）</p><p>所以这个时候就看下前人的related work，发现别人的work的效率不是很高。</p><p>for example：</p><ol><li>a gradient intensity map, driven by geometric constraints between skeletal pixels and edge fragments （根据骨架像素和边缘的几何约束的梯度来进行骨架的计算，但是就是在没有物体形状和位置信息的情况下是无法轻松的处理复杂数据的）</li><li>improved learning-based methods（虽然较上述的方法有了极大的提高——CNN给行业带来太大的巨变了，但是思想还是一个二分类像素的问题，但是在多个物体的场景，或者物体背景也有像素点时就会出现不太好的情况）</li></ol><p>于是在learning-based methods基础上提出了flux的想法：</p><p>​        training a convolutional neural network to predict a two-dimensional vector field encoding the flux representation.         Then recover the skeleton from the flux representation, which captures the position of skeletal pixels relative to         semantically meaningful entities, resulting in precise skeleton detection.（给每个像素点一个向量，也就是flux，然        后通过向量来计算骨架，相当于给分类增加了信息量。）</p><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>As I mentioned in the above, the work is divide into two methods.</p><ul><li><p> The first method is a gradient intensity map, which includes a lot of different ways to achieve it , such as <a href="https://arxiv.org/abs/1703.08628">AMAT</a>. However, they all haven’t very great performance.（因为没有太大参考价值，所以不做详细描述）</p></li><li><p>The second method is the learning-based method.</p></li></ul><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><p><strong>The main idea :</strong></p><p>we define a two-dimensional unit flux vector pointing to the nearest skeleton pixel, generating a flux vector field. Within this representation, the object skeleton corresponds to pixels where the net inward flux is positive, following the motivation behind past flux-based methods for skeletonizing binary objects. We then use a CNN to learn the spatial context flux, via a pixel-wise regression task in place of binary classification（给上下文空间的像素点定义一个向量指向最近的骨架像素，这个向量就是通量，然后使用CNN来学习flux，通过每个像素的回归来代替二分类）</p><p>将像素点分为三类：</p><ul><li>Rs: skeleton pixels</li><li>Rb: background pixels</li><li>Rc: skeleton spatial context. they are obtained by dilating the binary skeleton map with a disk of radius r, and subtract skeleton pixels Rs </li></ul><p>Then define skeleton points as the local flux minima, or, alternatively, as sinks “absorbing” flux from nearby points，This approach leads to more robust localization and better connectivity between skeletal branches</p><p>于是通量F(p)为</p><p><img src="/2022/01/26/deepflux/1.png"></p><p>Use the color wheel to show the correspondence between color and orientation:</p><p><img src="/2022/01/26/deepflux/flux2.png"></p><p> <strong>Advantages:</strong></p><p>​        利用了每个候选像素周围的邻域内通量预测之间的一致性。如果骨架位置发生变化，其周围的通量场也会发生显著变化。一个有益的副作用是，该方法不直接依赖于更深层CNN层产生的粗反应来定位更大尺度的骨骼，这进一步减少了定位误差。这些特性使骨架点的定位更加稳健，特别是在结扎点周围，更不容易出现缺口、不连续和不规则性。</p><p>Then we can see the figure:<br><img src="/2022/01/26/deepflux/3.png"></p><p>it can recover medial axes reflecting object part symmetries by localizing points with high inward flux (followed by a morphological closing), or by using additional convolution layers, which makes the entire pipeline end-to-end trainable（就是说通量这个信息量可以使用额外的卷积层来学习，或者使用通量大的信息进行定位）</p><p><strong>Network structure:</strong></p><p>The model is divided by 5 modules:</p><ol><li>a backbone network used to extract 3-dimension feature map</li><li>ASPP module</li><li>a multi-stage feature fusion module</li><li>a flux regression and skeleton classification by convolution and up-sampling module</li><li>an optional skeleton scale prediction branch</li></ol><p>The backbone network:VGG16,but discard the last pooling and the fully connected layers,and define it as DeepFlux-VGG16.(The first module)</p><p>The main reason of using ASPP module is that we need a wider receptive  field,we should guarantee the receptive field is wider than the biggest radius in the entity of input image,but the receptive field of VGG is 196, it’s not enough.</p><p>The main way to achieve the ASPP module  is adding four parallel atrous convolutional layers with 3 × 3 kernels but different atrous rates (2, 4, 8, 16) to the last layer of the backbone, followed by a concatenation along the channel dimension. Then we obtain  a feature map with a theoretical receptive field of 708 and it’s enough for the task.(The second module)</p><p>ASPP解释：</p><p><img src="/2022/01/26/deepflux/ASPP2.png"></p><p>就是说保持了输出的尺寸，让出来的特征变大也就是感受野变大。</p><p>In the following I show the way to calculate the receptive field:</p><p>![](deepflux/receptive field.png)</p><p>we fuse the feature maps from side outputs at conv3, conv4, conv5, and ASPP layers, after convolving them with  1 × 1 kernel, we adjust them to the dimension of conv3(feature maps at different levels have different spatial resolutions), then predict the learnt flux field and use the  bilinear interpolation to  up-sample it to the  dimension of input image ,which is  a 2-channel response map, corresponding to flux predictions Fˆ(p) for every pixel p in the image.(The third module and the fourth module,用双线性插值进行上采样来resize确定像素点的F(p)值)</p><p>上采样方法(up-sample)：<br><img src="/2022/01/26/deepflux/up-sample.png"></p><p>Then we should extract the skeleton from the 2-channel response map, and we have two approaches to make it.</p><ol><li>for a given pixel,the direction of it  is binned into one of 8 directions, pointing to one of the 8 neighbors. Pixels close to the real object skeleton should have a high inward flux,because the skeleton is  the local flux minima. Finally, we apply a morphological dilation with a disk structuring element of radius k1, followed by a morphological erosion with a disk of radius k2, to group quench points together and produce the object skeleton.(DeepFlux-P，因为骨架点是局部最小值，所以利用flux值有较大向内通量的特性找到骨架)</li><li>extend our network by plugging in three 3 × 3 convolutional layers (with 64-channel output for the first two layers), following the (up-sampled) flux field prediction layers, which output a pixel-wise skeleton confidence score to produce a binary skeleton(DeepFlux-E,也是默认值，使用3*3的卷积层然后预测flux得到一个score，通过score来找到骨架)</li></ol><p>具体过程如下：</p><p><img src="/2022/01/26/deepflux/ASPP.jpg"></p><p><strong>Loss:</strong></p><p>For the flux  field branch, due to a severe imbalance in the number of context and background pixels, we adopt a class-balancing strategy similar to <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf">the one</a> </p><p>and the loss is:</p><p><img src="/2022/01/26/deepflux/loss2.jpg"></p><p>The second branch uses a class-balanced cross-entropy loss function, which predicts skeleton probability scores from the predicted flux.</p><p>the loss is:</p><p><img src="/2022/01/26/deepflux/loss_cross.png"></p><p><img src="/2022/01/26/deepflux/explain.png"></p><p>The final training objective is given by summing the two loss terms.</p><p><img src="/2022/01/26/deepflux/final_los.jpg"></p><p>For the optional extra scale prediction branch, we use a smoothed-L1 loss for scale regression:</p><p><img src="/2022/01/26/deepflux/smooth_loss.jpg"></p><p>Then, the final loss is:                                              <img src="/2022/01/26/deepflux/final_loss.jpg"></p><p>Loss计算方法：<br><img src="/2022/01/26/deepflux/loss_cal.png"></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>这篇提出的方法缓解了以前方法的许多局限性(例如：定位不良)，在处理大空间尺度上的结扎点和物体骨架方面表现得非常好，同时速度也非常快。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The first blog is the detailed annotation of deepflux，which appears in IJCV. And its link is &lt;a href=&quot;https://doi.org/10.1007/s11263-021-</summary>
      
    
    
    
    
  </entry>
  
</feed>
